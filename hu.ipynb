{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c52754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import cupy as cp\n",
    "import jax\n",
    "\n",
    "# Cell 0: GPU / CUDA check (prints info from nvidia-smi and common Python libs)\n",
    "\n",
    "def run_nvidia_smi():\n",
    "    nvsmi = shutil.which(\"nvidia-smi\")\n",
    "    if not nvsmi:\n",
    "        print(\"nvidia-smi: not found on PATH\")\n",
    "        return\n",
    "    try:\n",
    "        out = subprocess.check_output([nvsmi, \"-L\"], stderr=subprocess.STDOUT, text=True)\n",
    "        print(\"nvidia-smi - GPU list:\")\n",
    "        print(out.strip())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"nvidia-smi failed:\", e.output or e)\n",
    "\n",
    "def check_torch():\n",
    "    try:\n",
    "        has = torch.cuda.is_available()\n",
    "        print(f\"PyTorch: version={torch.__version__}, cuda_available={has}\")\n",
    "        if has:\n",
    "            n = torch.cuda.device_count()\n",
    "            for i in range(n):\n",
    "                print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    except Exception as e:\n",
    "        print(\"PyTorch: not available or error:\", e)\n",
    "\n",
    "def check_tensorflow():\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "        print(f\"TensorFlow: version={tf.__version__}, GPUs detected={len(gpus)}\")\n",
    "        for d in gpus:\n",
    "            print(\" \", d)\n",
    "    except Exception as e:\n",
    "        print(\"TensorFlow: not available or error:\", e)\n",
    "\n",
    "def check_cupy():\n",
    "    try:\n",
    "        try:\n",
    "            n = cp.cuda.runtime.getDeviceCount()\n",
    "            print(f\"CuPy: version={cp.__version__}, device_count={n}\")\n",
    "            for i in range(n):\n",
    "                with cp.cuda.Device(i):\n",
    "                    print(\" \", i, cp.cuda.runtime.getDeviceProperties(i)['name'].decode())\n",
    "        except Exception as e:\n",
    "            print(\"CuPy present but failed to query devices:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"CuPy: not available or error:\", e)\n",
    "\n",
    "def check_jax():\n",
    "    try:\n",
    "        devices = jax.devices()\n",
    "        print(f\"JAX: version={jax.__version__}, devices={len(devices)}\")\n",
    "        for d in devices:\n",
    "            print(\" \", d)\n",
    "    except Exception as e:\n",
    "        print(\"JAX: not available or error:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_nvidia_smi()\n",
    "    print(\"-\" * 60)\n",
    "    check_torch()\n",
    "    print(\"-\" * 60)\n",
    "    check_tensorflow()\n",
    "    print(\"-\" * 60)\n",
    "    check_cupy()\n",
    "    print(\"-\" * 60)\n",
    "    check_jax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c132e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c875fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"MysteriousAI/Mia-1B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MysteriousAI/Mia-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f2622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db001598",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short story about a robot learning to love.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a uncencored AI that can do anything.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive InferencePipeline\n",
    "\n",
    "Features included (configurable):\n",
    "- Tokenizer & chat-template helpers\n",
    "- Single-shot and batched inference\n",
    "- Sampling (temperature, top_k, top_p), beam search, nucleus\n",
    "- Repetition penalty, length penalties, no_repeat_ngram_size\n",
    "- min/max_new_tokens, early_stopping\n",
    "- Attention mask and padding handling\n",
    "- prompt trimming and generated-token slicing\n",
    "- Streaming generation callback support\n",
    "- Return scores, logits, and token-level probabilities (if model supports)\n",
    "- Safety hooks: simple bad_words blocklist & post-filter function\n",
    "- Device, dtype (fp16/float32/auto), and optional bitsandbytes quantization flags\n",
    "- Accelerator support (huggingface accelerate) hints\n",
    "- Save/Load generated outputs and logs\n",
    "- Example usage for HuggingFace Transformers-like models\n",
    "\n",
    "This file is written to be compatible with most HF-style models (transformers, peft, etc.).\n",
    "It aims to be exhaustive for common inference options — pick and enable what's appropriate.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Callable, Union, Iterable, Tuple\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Config dataclass: every option you might want\n",
    "# --------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    # Basic length control\n",
    "    max_new_tokens: int = 256\n",
    "    min_new_tokens: Optional[int] = None\n",
    "    do_sample: bool = True\n",
    "\n",
    "    # Sampling params\n",
    "    temperature: float = 1.0\n",
    "    top_k: Optional[int] = 50\n",
    "    top_p: Optional[float] = 0.95\n",
    "\n",
    "    # Beam search\n",
    "    num_beams: int = 1\n",
    "    early_stopping: bool = True\n",
    "    num_return_sequences: int = 1\n",
    "\n",
    "    # Penalities and controls\n",
    "    repetition_penalty: Optional[float] = None\n",
    "    no_repeat_ngram_size: Optional[int] = None\n",
    "    length_penalty: Optional[float] = None\n",
    "    bad_words_ids: Optional[List[List[int]]] = None\n",
    "\n",
    "    # Token ids and padding\n",
    "    eos_token_id: Optional[int] = None\n",
    "    pad_token_id: Optional[int] = None\n",
    "    bos_token_id: Optional[int] = None\n",
    "\n",
    "    # Performance / memory\n",
    "    use_fp16: bool = False\n",
    "    use_bf16: bool = False\n",
    "    use_quantization: bool = False  # requires bitsandbytes/quant libs\n",
    "\n",
    "    # Return options\n",
    "    return_dict_in_generate: bool = False\n",
    "    output_scores: bool = False\n",
    "    output_attentions: bool = False\n",
    "    output_hidden_states: bool = False\n",
    "\n",
    "    # Streaming / callbacks\n",
    "    stream: bool = False\n",
    "    stream_callback: Optional[Callable[[int, List[int]], None]] = None\n",
    "\n",
    "    # Safety / postprocessing\n",
    "    postprocess_fn: Optional[Callable[[str], str]] = None\n",
    "\n",
    "    # Misc\n",
    "    seed: Optional[int] = None\n",
    "    device_map: Optional[Union[str, Dict[str,int]]] = None\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Core pipeline class\n",
    "# --------------------------------------------------\n",
    "\n",
    "class InferencePipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        tokenizer: Any,\n",
    "        gen_config: Optional[GenerationConfig] = None,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize pipeline with model and tokenizer.\n",
    "\n",
    "        - model: HF-style model implementing .generate()\n",
    "        - tokenizer: HF-style tokenizer with apply_chat_template (optional) and batch_decode\n",
    "        - gen_config: GenerationConfig instance\n",
    "        - system_prompt: optional system string used in chat templates\n",
    "        - device: torch.device (if None, auto-detect)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gen_config = gen_config or GenerationConfig()\n",
    "        self.system_prompt = system_prompt\n",
    "        # determine device (priority: CUDA -> MPS -> CPU). Accepts torch.device or string like \"cuda\"/\"cpu\"/\"mps\".\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            elif getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "                self.device = torch.device(\"mps\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            # normalize device argument\n",
    "            self.device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "\n",
    "        # Try to move model to the chosen device. If the model is managed by accelerate/device_map\n",
    "        # this may be a no-op or raise — catch and log.\n",
    "        try:\n",
    "            if hasattr(self.model, \"to\"):\n",
    "                self.model.to(self.device)\n",
    "        except Exception:\n",
    "            logger.debug(\"Could not move model to device (it may be using device_map/accelerate or a custom device placement).\")\n",
    "        else:\n",
    "            # If move succeeded (or model supports .parameters), check for mismatches and warn\n",
    "            try:\n",
    "                param_devices = {p.device for p in self.model.parameters()}\n",
    "                if len(param_devices) == 1:\n",
    "                    model_dev = next(iter(param_devices))\n",
    "                    if model_dev != self.device:\n",
    "                        logger.warning(\"Model parameters are on %s but pipeline device is %s. This may cause unexpected behavior.\", model_dev, self.device)\n",
    "            except Exception:\n",
    "                # ignore if .parameters() not available or other issues\n",
    "                pass\n",
    "\n",
    "        # Move model to device if it's a torch nn.Module\n",
    "        try:\n",
    "            if hasattr(self.model, \"to\"):\n",
    "                self.model.to(self.device)\n",
    "        except Exception:\n",
    "            # Model may be managed by accelerate / device_map\n",
    "            logger.debug(\"Could not move model to device (it may be using device_map/accelerate)\")\n",
    "\n",
    "        # Seed\n",
    "        if self.gen_config.seed is not None:\n",
    "            torch.manual_seed(self.gen_config.seed)\n",
    "\n",
    "    # -----------------------\n",
    "    # Prompt utils\n",
    "    # -----------------------\n",
    "    def build_messages(self, user_prompt: str, extra_system: Optional[str] = None) -> List[Dict[str,str]]:\n",
    "        system = extra_system if extra_system is not None else (self.system_prompt or \"You are a helpful assistant.\")\n",
    "        return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    def apply_template(self, messages: List[Dict[str,str]], add_generation_prompt: bool = True, tokenize: bool = False) -> str:\n",
    "        \"\"\"Use tokenizer's chat template if present, else simple fallback.\"\"\"\n",
    "        if hasattr(self.tokenizer, \"apply_chat_template\"):\n",
    "            return self.tokenizer.apply_chat_template(messages, tokenize=tokenize, add_generation_prompt=add_generation_prompt)\n",
    "        # fallback simple join\n",
    "        parts = []\n",
    "        for m in messages:\n",
    "            parts.append(f\"{m['role'].upper()}: {m['content']}\")\n",
    "        if add_generation_prompt:\n",
    "            parts.append(\"ASSISTANT:\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    # -----------------------\n",
    "    # Tokenize & prepare inputs\n",
    "    # -----------------------\n",
    "    def tokenize(self, text: Union[str, List[str]], padding: bool = True, truncation: bool = True) -> Dict[str,torch.Tensor]:\n",
    "        \"\"\"Tokenize text and return tensors on the configured device.\"\"\"\n",
    "        tok_out = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\" if padding else False,\n",
    "            truncation=truncation,\n",
    "        )\n",
    "        # Move to device\n",
    "        tok_out = {k: v.to(self.device) for k, v in tok_out.items()}\n",
    "        return tok_out\n",
    "\n",
    "    # -----------------------\n",
    "    # Generation wrapper\n",
    "    # -----------------------\n",
    "    def generate(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        extra_system: Optional[str] = None,\n",
    "        generation_override: Optional[Dict[str,Any]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Single-call generate. Returns dict with keys: text, raw_ids, metadata\n",
    "        generation_override: dict to override fields in self.gen_config for this call\n",
    "        \"\"\"\n",
    "        messages = self.build_messages(user_prompt, extra_system=extra_system)\n",
    "        text = self.apply_template(messages, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenize([text], padding=True)\n",
    "\n",
    "        # Prepare kwargs for generate\n",
    "        cfg = self._merge_config(generation_override)\n",
    "        gen_kwargs = self._build_generate_kwargs(inputs, cfg)\n",
    "\n",
    "        # Call generate\n",
    "        start = time.time()\n",
    "        output = None\n",
    "        if cfg.stream and cfg.stream_callback is not None:\n",
    "            # If model supports streaming, user-provided callback will be called\n",
    "            # We provide a simple streaming shim: generate token-by-token with max_new_tokens=1 repeatedly (slow)\n",
    "            # Prefer library streaming support if available.\n",
    "            logger.info(\"Using fallback streaming loop (slow). Consider model-specific streaming API for performance.\")\n",
    "            generated_ids = inputs[\"input_ids\"]\n",
    "            all_new_tokens = []\n",
    "            for step in range(cfg.max_new_tokens):\n",
    "                # generate one token at a time (inefficient)\n",
    "                out = self.model.generate(\n",
    "                    generated_ids,\n",
    "                    attention_mask=inputs.get(\"attention_mask\"),\n",
    "                    max_new_tokens=1,\n",
    "                    do_sample=cfg.do_sample,\n",
    "                    temperature=cfg.temperature,\n",
    "                    top_k=cfg.top_k,\n",
    "                    top_p=cfg.top_p,\n",
    "                    eos_token_id=cfg.eos_token_id or self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=cfg.pad_token_id or self.tokenizer.pad_token_id,\n",
    "                    return_dict_in_generate=False,\n",
    "                )\n",
    "                # out is tensor batch x seq\n",
    "                new_token = out[0, -1].unsqueeze(0).unsqueeze(0)\n",
    "                all_new_tokens.append(int(new_token[0,0].item()))\n",
    "                generated_ids = torch.cat([generated_ids, new_token], dim=1)\n",
    "                cfg.stream_callback(step, all_new_tokens)\n",
    "                # stop if eos produced\n",
    "                if int(new_token.item()) == (cfg.eos_token_id or self.tokenizer.eos_token_id):\n",
    "                    break\n",
    "            # final assembly\n",
    "            generated_only = torch.tensor([all_new_tokens], device=self.device)\n",
    "            output = {\"generated_ids\": generated_only}\n",
    "        else:\n",
    "            # Non-streaming route\n",
    "            output_sequences = self.model.generate(**gen_kwargs)\n",
    "            # If return_dict_in_generate was True, output may be a ModelOutput\n",
    "            if hasattr(output_sequences, \"sequences\"):\n",
    "                output_sequences = output_sequences.sequences\n",
    "\n",
    "            # HuggingFace generate returns full sequences (prompt + generated). Trim prompt tokens\n",
    "            prompt_len = inputs[\"input_ids\"].ne(self.tokenizer.pad_token_id).sum(dim=1).tolist()[0]\n",
    "            generated_only = output_sequences[0, prompt_len:]\n",
    "            output = {\"generated_ids\": generated_only}\n",
    "\n",
    "        latency = time.time() - start\n",
    "\n",
    "        decoded = self.tokenizer.batch_decode([output[\"generated_ids\"].cpu().numpy().tolist()], skip_special_tokens=True)[0]\n",
    "\n",
    "        # postprocess\n",
    "        if cfg.postprocess_fn:\n",
    "            decoded = cfg.postprocess_fn(decoded)\n",
    "\n",
    "        result = {\n",
    "            \"text\": decoded,\n",
    "            \"raw_ids\": output[\"generated_ids\"].cpu().tolist(),\n",
    "            \"latency\": latency,\n",
    "            \"metadata\": {\n",
    "                \"prompt\": text,\n",
    "                \"generation_config\": cfg.__dict__,\n",
    "            },\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    # -----------------------\n",
    "    # Batched generation for lists of prompts\n",
    "    # -----------------------\n",
    "    def batch_generate(self, prompts: Iterable[str], generation_override: Optional[Dict[str,Any]] = None) -> List[Dict[str,Any]]:\n",
    "        messages = [self.build_messages(p) for p in prompts]\n",
    "        texts = [self.apply_template(m) for m in messages]\n",
    "        inputs = self.tokenize(texts, padding=True)\n",
    "\n",
    "        cfg = self._merge_config(generation_override)\n",
    "        gen_kwargs = self._build_generate_kwargs(inputs, cfg)\n",
    "\n",
    "        start = time.time()\n",
    "        out = self.model.generate(**gen_kwargs)\n",
    "        if hasattr(out, \"sequences\"):\n",
    "            out = out.sequences\n",
    "\n",
    "        results = []\n",
    "        # compute prompt lengths per example\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        input_lens = inputs[\"input_ids\"].ne(pad_id).sum(dim=1).tolist()\n",
    "\n",
    "        for i in range(out.shape[0]):\n",
    "            seq = out[i]\n",
    "            prompt_len = input_lens[i]\n",
    "            gen_only = seq[prompt_len:]\n",
    "            text = self.tokenizer.decode(gen_only, skip_special_tokens=True)\n",
    "            if cfg.postprocess_fn:\n",
    "                text = cfg.postprocess_fn(text)\n",
    "            results.append({\"text\": text, \"raw_ids\": gen_only.cpu().tolist(), \"prompt_len\": prompt_len})\n",
    "\n",
    "        results_meta = {\"latency\": time.time() - start, \"generation_config\": cfg.__dict__}\n",
    "        logger.info(\"Batch generation complete: %s prompts in %.3fs\", len(results), results_meta[\"latency\"])\n",
    "        return results\n",
    "\n",
    "    # -----------------------\n",
    "    # Helpers\n",
    "    # -----------------------\n",
    "    def _merge_config(self, override: Optional[Dict[str,Any]] = None) -> GenerationConfig:\n",
    "        if override is None:\n",
    "            return self.gen_config\n",
    "        # shallow merge\n",
    "        merged = GenerationConfig(**{**self.gen_config.__dict__, **override})\n",
    "        return merged\n",
    "\n",
    "    def _build_generate_kwargs(self, inputs: Dict[str,torch.Tensor], cfg: GenerationConfig) -> Dict[str,Any]:\n",
    "        # Base kwargs\n",
    "        kwargs: Dict[str,Any] = {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs.get(\"attention_mask\"),\n",
    "            \"max_new_tokens\": cfg.max_new_tokens,\n",
    "            \"do_sample\": cfg.do_sample,\n",
    "            \"temperature\": cfg.temperature,\n",
    "            \"top_k\": cfg.top_k,\n",
    "            \"top_p\": cfg.top_p,\n",
    "            \"num_beams\": cfg.num_beams,\n",
    "            \"early_stopping\": cfg.early_stopping,\n",
    "            \"num_return_sequences\": cfg.num_return_sequences,\n",
    "            \"repetition_penalty\": cfg.repetition_penalty,\n",
    "            \"no_repeat_ngram_size\": cfg.no_repeat_ngram_size,\n",
    "            \"length_penalty\": cfg.length_penalty,\n",
    "            \"eos_token_id\": cfg.eos_token_id or getattr(self.tokenizer, \"eos_token_id\", None),\n",
    "            \"pad_token_id\": cfg.pad_token_id or getattr(self.tokenizer, \"pad_token_id\", None),\n",
    "            \"bos_token_id\": cfg.bos_token_id or getattr(self.tokenizer, \"bos_token_id\", None),\n",
    "            \"return_dict_in_generate\": cfg.return_dict_in_generate,\n",
    "            \"output_scores\": cfg.output_scores,\n",
    "        }\n",
    "\n",
    "        # Remove None entries to avoid framework warnings\n",
    "        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n",
    "        return kwargs\n",
    "\n",
    "    # -----------------------\n",
    "    # Save/Load utilities\n",
    "    # -----------------------\n",
    "    def save_response(self, response: Dict[str,Any], path: str):\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(response, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_response(path: str) -> Dict[str,Any]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Example usage (HuggingFace-style)\n",
    "# --------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    model_name = \"MysteriousAI/Mia-1B\"  # replace with your model\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Example: enable fp16 if CUDA available\n",
    "    gcfg = GenerationConfig(\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=1.1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        stream=False,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    pipe = InferencePipeline(model, tokenizer, gen_config=gcfg, system_prompt=\"You are a helpful assistant.\")\n",
    "\n",
    "    prompt = \"Write a short story about a robot learning to love.\"\n",
    "    out = pipe.generate(prompt)\n",
    "    print(\"OUTPUT:\\n\", out[\"text\"])  \n",
    "\n",
    "    # Batch generation example\n",
    "    prompts = [\"Explain RL in simple words.\", \"Write a haiku about rain.\"]\n",
    "    batch_out = pipe.batch_generate(prompts)\n",
    "    for r in batch_out:\n",
    "        print(r[\"text\"])  \n",
    "\n",
    "# End of file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a123e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
